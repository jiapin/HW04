{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from opencc import OpenCC\n",
    "import jieba\n",
    "import re\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = '[）\\（\\：\\·\\，\\。\\“ \\”\\?\\？\\」\\「\\……\\、\\《\\》\\；\\)\\(]'\n",
    "file = ['AA','AB','AC','AD','AE','AF','AG','AH','AI','AJ','AK','AL']\n",
    "cc = OpenCC('s2t')\n",
    "\n",
    "def open_wiki():\n",
    "    result =[]\n",
    "    for a in file[0:4]:\n",
    "        for i in range(100):\n",
    "            for line in open('wiki_zh/{}/wiki_{}'.format(a,str(i).zfill(2))):\n",
    "                data = json.loads(line)\n",
    "                value = re.sub(r,'',data['text'])\n",
    "                result.append(value)\n",
    "\n",
    "    for a in file[5:8]:\n",
    "        for i in range(100):\n",
    "            for line in open('wiki_zh/{}/wiki_{}'.format(a,str(i).zfill(2))): \n",
    "                data = json.loads(line)\n",
    "                value = re.sub(r,'',data['text'])\n",
    "                result.append(value)\n",
    "\n",
    "    for a in file[9:11]:\n",
    "        for i in range(100):\n",
    "            for line in open('wiki_zh/{}/wiki_{}'.format(a,str(i).zfill(2))):\n",
    "                data = json.loads(line)\n",
    "                value = re.sub(r,'',data['text'])\n",
    "                result.append(value)\n",
    "\n",
    "    for i in range(74):\n",
    "        for line in open('wiki_zh/{}/wiki_{}'.format('AM',str(i).zfill(2))):\n",
    "            data = json.loads(line)\n",
    "            value = re.sub(r,'',data['text'])\n",
    "            result.append(value)\n",
    "            \n",
    "    return result\n",
    "    \n",
    "result = open_wiki()\n",
    "\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chine(result):\n",
    "    value = []\n",
    "    for line in result:\n",
    "        a = cc.convert(line)\n",
    "        value.append(a)\n",
    "    return value\n",
    "\n",
    "value=chine(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_list():       \n",
    "    with open('wiki_zh/cn_stopwords.txt') as f: \n",
    "        stopword_list = [word.strip('\\n') for word in f.readlines()]\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macktxt(value):\n",
    "    stop = get_stopword_list()\n",
    "    f = open('test1.txt','w')\n",
    "\n",
    "    print(len(value))\n",
    "\n",
    "    jieba.set_dictionary('/usr/local/lib/python3.9/site-packages/jieba/dict.txt')\n",
    "    jieba.load_userdict(\"txt2.txt\")  \n",
    "\n",
    "    for line in value :  \n",
    "        data = jieba.lcut(line,cut_all=False)\n",
    "        for val in data:\n",
    "            if val not in stop:\n",
    "                f.write(val+' ')\n",
    "        # f.write('\\n')\n",
    "    f.close\n",
    "\n",
    "macktxt(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2():\n",
    "    train_data = word2vec.LineSentence('test.txt')\n",
    "    # seed = 666\n",
    "    sg = 1\n",
    "    # window_size = 10\n",
    "    vector_size = 300\n",
    "    # min_count = 1\n",
    "    workers = 4\n",
    "    epochs = 5\n",
    "    # batch_words = 10000\n",
    "    model = word2vec.Word2Vec(\n",
    "        train_data,\n",
    "        # min_count=min_count,\n",
    "        vector_size=vector_size,\n",
    "        workers=workers,\n",
    "        epochs=epochs,\n",
    "        # window=window_size,\n",
    "        sg=sg,\n",
    "        # seed=seed,\n",
    "        # batch_words=batch_words\n",
    "    )\n",
    "    model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(string):\n",
    "    model = word2vec.Word2Vec.load('word2vec.model')\n",
    "    # print(model.wv['生物'].shape)\n",
    "\n",
    "    for item in model.wv.most_similar(string,topn=20):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2()\n",
    "print('訓練成功！')\n",
    "test('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
